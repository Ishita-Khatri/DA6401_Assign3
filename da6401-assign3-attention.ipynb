{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11836867,"sourceType":"datasetVersion","datasetId":7436663}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport wandb\nimport torch.nn.functional as F","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:59:19.378501Z","iopub.execute_input":"2025-05-19T10:59:19.379105Z","iopub.status.idle":"2025-05-19T10:59:19.382973Z","shell.execute_reply.started":"2025-05-19T10:59:19.379084Z","shell.execute_reply":"2025-05-19T10:59:19.382169Z"}},"outputs":[],"execution_count":93},{"cell_type":"code","source":"def load_dakshina_lexicon_pairs(filepath):\n    pairs=[]\n    with open(filepath, 'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue  # skip empty lines\n            parts = line.split('\\t')\n            if len(parts) != 3:\n                continue  # skip malformed lines\n            devanagari_word, latin_word,_ = parts\n            pairs.append((latin_word, devanagari_word))  # reverse order\n    return pairs","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:59:19.383976Z","iopub.execute_input":"2025-05-19T10:59:19.384181Z","iopub.status.idle":"2025-05-19T10:59:19.400400Z","shell.execute_reply.started":"2025-05-19T10:59:19.384166Z","shell.execute_reply":"2025-05-19T10:59:19.399734Z"}},"outputs":[],"execution_count":94},{"cell_type":"code","source":"def build_vocab(pairs, add_special_tokens=True):\n    input_chars = set() # to ensure no repeated characters\n    output_chars = set()\n\n    # Collect unique characters from Latin (input) and Devanagari (output)\n    for latin_word, devnagari_word in pairs:\n        input_chars.update(list(latin_word))\n        output_chars.update(list(devnagari_word))\n\n    # Sort to keep it consistent\n    input_chars = sorted(list(input_chars))\n    output_chars = sorted(list(output_chars))\n\n    # Add special tokens\n    special_tokens = ['<pad>', '<sos>', '<eos>', '<unk>'] if add_special_tokens else []\n\n    input_vocab = special_tokens + input_chars\n    output_vocab = special_tokens + output_chars\n\n    # Create dictionaries\n    input_char2idx = {ch: idx for idx, ch in enumerate(input_vocab)}\n    input_idx2char = {idx: ch for ch, idx in input_char2idx.items()}\n\n    output_char2idx = {ch: idx for idx, ch in enumerate(output_vocab)}\n    output_idx2char = {idx: ch for ch, idx in output_char2idx.items()}\n\n    return input_char2idx, input_idx2char, output_char2idx, output_idx2char","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:59:19.401521Z","iopub.execute_input":"2025-05-19T10:59:19.401752Z","iopub.status.idle":"2025-05-19T10:59:19.415505Z","shell.execute_reply.started":"2025-05-19T10:59:19.401737Z","shell.execute_reply":"2025-05-19T10:59:19.415029Z"}},"outputs":[],"execution_count":95},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, input_vocab_size, embed_size, hidden_size, num_encoder_layers=1, cell_type='lstm', dropout=0.0):\n        super(Encoder, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_encoder_layers = num_encoder_layers\n        self.cell_type = cell_type.lower()\n        \n        # Embedding layer\n        self.embedding = nn.Embedding(input_vocab_size, embed_size)\n        \n        # RNN layer\n        if self.cell_type == 'lstm':\n            self.rnn = nn.LSTM(\n                embed_size, hidden_size, num_encoder_layers,\n                batch_first=True, dropout=dropout if num_encoder_layers > 1 else 0\n            )\n        elif self.cell_type == 'gru':\n            self.rnn = nn.GRU(\n                embed_size, hidden_size, num_encoder_layers,\n                batch_first=True, dropout=dropout if num_encoder_layers > 1 else 0\n            )\n        else:  # default to RNN\n            self.rnn = nn.RNN(\n                embed_size, hidden_size, num_encoder_layers,\n                batch_first=True, dropout=dropout if num_encoder_layers > 1 else 0\n            )\n    def forward(self, input_seq, lengths):\n        \"\"\"\n        Forward pass for encoder\n        \n        Args:\n            input_seq: Input sequence tensor [batch_size, max_seq_len]\n            lengths: Actual lengths of input sequences (tensor)\n            \n        Returns:\n            outputs: Unpacked encoder outputs [batch_size, max_seq_len, hidden_size]\n            hidden: Hidden state for decoder initialization\n        \"\"\"\n        batch_size = input_seq.size(0)\n        \n        # Important: ensure lengths is on CPU before using it\n        if lengths.is_cuda:\n            lengths = lengths.cpu()\n        \n        # Convert input to embeddings\n        embedded = self.embedding(input_seq)  # [batch_size, seq_len, embed_size]\n        \n        # Pack padded sequence\n        packed = nn.utils.rnn.pack_padded_sequence(\n            embedded, lengths, batch_first=True, enforce_sorted=False\n        )\n        \n        # Process with RNN\n        if self.cell_type == 'lstm':\n            packed_outputs, (hidden, cell) = self.rnn(packed)\n            outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)\n            return outputs, (hidden, cell)\n        else:\n            packed_outputs, hidden = self.rnn(packed)\n            outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)\n            return outputs, hidden","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:59:19.416202Z","iopub.execute_input":"2025-05-19T10:59:19.416745Z","iopub.status.idle":"2025-05-19T10:59:19.438720Z","shell.execute_reply.started":"2025-05-19T10:59:19.416729Z","shell.execute_reply":"2025-05-19T10:59:19.438231Z"}},"outputs":[],"execution_count":96},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, hidden_size):\n        super(Attention, self).__init__()\n        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n        self.score = nn.Linear(hidden_size, 1, bias=False)\n\n    def forward(self, decoder_hidden, encoder_outputs):\n        # decoder_hidden: [batch_size, hidden_size]\n        # encoder_outputs: [batch_size, seq_len, hidden_size]\n        seq_len = encoder_outputs.size(1)\n        hidden = decoder_hidden.unsqueeze(1).repeat(1, seq_len, 1)\n        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n        scores = self.score(energy).squeeze(2)\n        attn_weights = torch.softmax(scores, dim=1)\n        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n        return context, attn_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:59:19.439862Z","iopub.execute_input":"2025-05-19T10:59:19.440128Z","iopub.status.idle":"2025-05-19T10:59:19.459046Z","shell.execute_reply.started":"2025-05-19T10:59:19.440113Z","shell.execute_reply":"2025-05-19T10:59:19.458551Z"}},"outputs":[],"execution_count":97},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, output_vocab_size, embed_size, hidden_size, num_decoder_layers=1, cell_type='lstm', dropout=0.0):\n        super(Decoder, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_decoder_layers = num_decoder_layers\n        self.output_vocab_size = output_vocab_size\n        self.cell_type = cell_type.lower()\n\n        self.embedding = nn.Embedding(output_vocab_size, embed_size)\n        self.attention = Attention(hidden_size)\n\n        self.rnn = nn.LSTM(embed_size + hidden_size, hidden_size, num_decoder_layers,\n                           batch_first=True, dropout=dropout if num_decoder_layers > 1 else 0) if self.cell_type == 'lstm' else \\\n                   nn.GRU(embed_size + hidden_size, hidden_size, num_decoder_layers,\n                          batch_first=True, dropout=dropout if num_decoder_layers > 1 else 0)\n\n        self.out = nn.Linear(hidden_size, output_vocab_size)\n\n    def forward(self, input_seq, hidden, encoder_outputs):\n        # input_seq: [batch_size, 1]\n        embedded = self.embedding(input_seq).squeeze(1)  # [batch_size, embed_size]\n\n        if self.cell_type == 'lstm':\n            decoder_hidden = hidden[0][-1]  # last layer's hidden state\n        else:\n            decoder_hidden = hidden[-1]\n\n        context, attn_weights = self.attention(decoder_hidden, encoder_outputs)  # [batch_size, hidden_size]\n\n        rnn_input = torch.cat((embedded, context), dim=1).unsqueeze(1)  # [batch_size, 1, embed+context]\n\n        if self.cell_type == 'lstm':\n            output, (hidden, cell) = self.rnn(rnn_input, hidden)\n            output = self.out(output)  # [batch_size, 1, vocab_size]\n            return output, (hidden, cell), attn_weights\n        else:\n            output, hidden = self.rnn(rnn_input, hidden)\n            output = self.out(output)  # [batch_size, 1, vocab_size]\n            return output, hidden, attn_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:59:19.496653Z","iopub.execute_input":"2025-05-19T10:59:19.496850Z","iopub.status.idle":"2025-05-19T10:59:19.504110Z","shell.execute_reply.started":"2025-05-19T10:59:19.496835Z","shell.execute_reply":"2025-05-19T10:59:19.503356Z"}},"outputs":[],"execution_count":98},{"cell_type":"code","source":"class TransliterationDataset(Dataset):\n    def __init__(self, pairs, input_char2idx, output_char2idx):\n        \n        '''\n        pairs: list of (latin_word, devnagari_word) tuples.\n        input_char2idx: dictionary mapping each Latin character to an index.\n        output_char2idx: dictionary mapping each Devanagari character to an index.\n        '''\n            \n        self.pairs = pairs\n        self.input_char2idx = input_char2idx\n        self.output_char2idx = output_char2idx\n\n    # This converts a word into a list of token indices, e.g., India -> [8,13,3,8,0]\n    def encode_word(self, word, char2idx, add_sos_eos=False):\n        tokens = [char2idx.get(c, char2idx['<unk>']) for c in word]\n        if add_sos_eos:\n            tokens = [char2idx['<sos>']] + tokens + [char2idx['<eos>']]\n        return tokens\n\n    #  Give the total number of latin, devnagri pairs in the dataset\n    def __len__(self): \n        return len(self.pairs)\n\n    # This takes the index of the word in latin and gets the latin, devnagri pair. \n        # Then, it converts each word to list of indices and gives the pair of list of indices\n    def __getitem__(self, idx):\n        latin, devnagari = self.pairs[idx]\n        input_ids = self.encode_word(latin, self.input_char2idx)\n        target_ids = self.encode_word(devnagari, self.output_char2idx, add_sos_eos=True)\n        return input_ids, target_ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:59:19.505196Z","iopub.execute_input":"2025-05-19T10:59:19.505761Z","iopub.status.idle":"2025-05-19T10:59:19.524511Z","shell.execute_reply.started":"2025-05-19T10:59:19.505745Z","shell.execute_reply":"2025-05-19T10:59:19.523815Z"}},"outputs":[],"execution_count":99},{"cell_type":"code","source":"#  Adds pad tokens, given the sequnece, maximum length and pad-token\ndef pad_seq(seq, max_len, pad_token):\n    return seq + [pad_token] * (max_len - len(seq))\n\ndef collate_fn(batch):\n    '''\n    batch: List of tuples [(input1, target1), (input2, target2), ...]\n\n    '''\n    input_seqs, target_seqs = zip(*batch)\n\n    input_max_len = max(len(seq) for seq in input_seqs)\n    target_max_len = max(len(seq) for seq in target_seqs)\n\n    # Adds padding for seqeuces so that sequence length = maximum sequence length in the batch. \n    # Now all sequenes in the batch are of same length \n    input_padded = [pad_seq(seq, input_max_len, pad_token=input_char2idx['<pad>']) for seq in input_seqs]\n    target_padded = [pad_seq(seq, target_max_len, pad_token=output_char2idx['<pad>']) for seq in target_seqs]\n\n    input_tensor = torch.tensor(input_padded, dtype=torch.long)\n    target_tensor = torch.tensor(target_padded, dtype=torch.long)\n\n    input_lengths = torch.tensor([len(seq) for seq in input_seqs])\n    target_lengths = torch.tensor([len(seq) for seq in target_seqs])\n\n    return input_tensor, input_lengths, target_tensor, target_lengths","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:59:19.525238Z","iopub.execute_input":"2025-05-19T10:59:19.525457Z","iopub.status.idle":"2025-05-19T10:59:19.542755Z","shell.execute_reply.started":"2025-05-19T10:59:19.525439Z","shell.execute_reply":"2025-05-19T10:59:19.542116Z"}},"outputs":[],"execution_count":100},{"cell_type":"code","source":"sweep_config = {\n    'method': 'bayes',  # Could also be 'random' or 'grid'\n    'metric': {\n        'name': 'token_accuracy',\n        'goal': 'maximize'\n    },\n    'parameters': {\n        'embed_size': {'values': [16, 32, 64]},\n        'hidden_size': {'values': [16, 32, 64]},\n        'cell_type': {'values': ['RNN', 'GRU', 'LSTM']},\n        'dropout': {'values': [0.3, 0.4, 0.5]},\n        'batch_size': {'values': [128, 256, 512]},\n        'learning_rate': {'values': [5e-3, 1e-3, 5e-4]},\n        'beam_size': {'values': [3, 4, 5]}\n    },\n    'early_terminate': {\n        'type': 'hyperband',\n        'min_iter': 7\n    }\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:59:19.544219Z","iopub.execute_input":"2025-05-19T10:59:19.544751Z","iopub.status.idle":"2025-05-19T10:59:19.559139Z","shell.execute_reply.started":"2025-05-19T10:59:19.544730Z","shell.execute_reply":"2025-05-19T10:59:19.558620Z"}},"outputs":[],"execution_count":101},{"cell_type":"code","source":"import wandb\nwandb.login(key=\"70a00ae1607c730fb9cd50b1268b191bec7a2901\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:59:19.559735Z","iopub.execute_input":"2025-05-19T10:59:19.559921Z","iopub.status.idle":"2025-05-19T10:59:19.701178Z","shell.execute_reply.started":"2025-05-19T10:59:19.559908Z","shell.execute_reply":"2025-05-19T10:59:19.700436Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":102,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":102},{"cell_type":"code","source":"filepath = \"/kaggle/input/dakshina-dataset-v1-0/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\npairs = load_dakshina_lexicon_pairs(filepath)\n\ninput_char2idx, input_idx2char, output_char2idx, output_idx2char = build_vocab(pairs)\n\nprint(\"Latin char2idx:\", list(input_char2idx.items())[:5])\nprint(\"Devanagari idx2char:\", list(output_idx2char.items())[:5])\n\nprint(len(list(output_char2idx.keys())))\n\ndataset = TransliterationDataset(pairs, input_char2idx, output_char2idx)\n\nfilepath_val = \"/kaggle/input/dakshina-dataset-v1-0/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\"\npairs_val = load_dakshina_lexicon_pairs(filepath_val)\ndataset_val = TransliterationDataset(pairs_val, input_char2idx, output_char2idx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:59:19.701835Z","iopub.execute_input":"2025-05-19T10:59:19.702045Z","iopub.status.idle":"2025-05-19T10:59:19.782436Z","shell.execute_reply.started":"2025-05-19T10:59:19.702030Z","shell.execute_reply":"2025-05-19T10:59:19.781855Z"}},"outputs":[{"name":"stdout","text":"Latin char2idx: [('<pad>', 0), ('<sos>', 1), ('<eos>', 2), ('<unk>', 3), ('a', 4)]\nDevanagari idx2char: [(0, '<pad>'), (1, '<sos>'), (2, '<eos>'), (3, '<unk>'), (4, 'ँ')]\n67\n","output_type":"stream"}],"execution_count":103},{"cell_type":"code","source":"def train():\n    wandb.init()\n    config = wandb.config\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Initialize encoder and decoder\n    encoder = Encoder(\n        input_vocab_size=len(input_char2idx),\n        embed_size=config.embed_size,\n        hidden_size=config.hidden_size,\n        num_encoder_layers=1,\n        cell_type=config.cell_type,\n        dropout=config.dropout\n    ).to(device)\n\n    decoder = Decoder(\n        output_vocab_size=len(output_char2idx),\n        embed_size=config.embed_size,\n        hidden_size=config.hidden_size,\n        num_decoder_layers=1,\n        cell_type=config.cell_type,\n        dropout=config.dropout\n    ).to(device)\n\n    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=config.learning_rate, weight_decay=1e-5)\n    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=config.learning_rate, weight_decay=1e-5)\n    criterion = nn.CrossEntropyLoss(ignore_index=output_char2idx['<pad>'])\n\n    dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn)\n    dataloader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, collate_fn=collate_fn)\n\n    num_epochs = 20\n    for epoch in range(num_epochs):\n        # ======== TRAINING ========\n        encoder.train()\n        decoder.train()\n        total_loss = 0\n\n        with tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\") as pbar:\n            for input_tensor, input_lengths, target_tensor, target_lengths in pbar:\n                input_tensor = input_tensor.to(device)\n                target_tensor = target_tensor.to(device)\n\n                encoder_optimizer.zero_grad()\n                decoder_optimizer.zero_grad()\n\n                encoder_outputs, encoder_hidden = encoder(input_tensor, input_lengths)\n                decoder_input = target_tensor[:, 0].unsqueeze(1)  # <sos>\n                decoder_hidden = encoder_hidden\n\n                loss = 0\n                max_target_len = target_tensor.size(1)\n\n                for t in range(1, max_target_len):\n                    decoder_output, decoder_hidden,_= decoder(decoder_input, decoder_hidden,encoder_outputs)\n                    output = decoder_output.squeeze(1)\n                    # print(f'output = {output}')\n                    # print(f'target tensor = {target_tensor[:,t]}')\n                    loss += criterion(output, target_tensor[:, t])\n                    decoder_input = target_tensor[:, t].unsqueeze(1)  # Teacher forcing\n\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(encoder.parameters(), max_norm=1.0)\n                torch.nn.utils.clip_grad_norm_(decoder.parameters(), max_norm=1.0)\n\n                encoder_optimizer.step()\n                decoder_optimizer.step()\n\n                total_loss += loss.item() / (max_target_len - 1)\n\n        avg_loss = total_loss / len(dataloader)\n        print(f\"Epoch {epoch+1}/{num_epochs} Train Loss: {avg_loss:.4f}\")\n\n        # ======== VALIDATION ========\n        encoder.eval()\n        decoder.eval()\n        correct_sequences = 0\n        total_sequences = 0\n        correct_tokens = 0\n        total_tokens = 0\n        beam_width = config.beam_size  # You can change this\n        \n        with torch.no_grad():\n            for input_tensor, input_lengths, target_tensor, target_lengths in dataloader_val:\n                input_tensor = input_tensor.to(device)\n                target_tensor = target_tensor.to(device)\n        \n                encoder_outputs, encoder_hidden = encoder(input_tensor, input_lengths)\n                max_target_len = target_tensor.size(1)\n                total_sequences += 1\n        \n                # Beam is a list of tuples: (sequence_so_far, cumulative_log_prob, decoder_hidden)\n                beam = [([output_char2idx['<sos>']], 0.0, encoder_hidden)]\n        \n                completed_sequences = []\n        \n                for _ in range(1, max_target_len):\n                    new_beam = []\n                    for seq, score, hidden in beam:\n                        decoder_input = torch.tensor([[seq[-1]]], device=device)\n                        decoder_output, hidden_next,_ = decoder(decoder_input, hidden,encoder_outputs)\n                        log_probs = F.log_softmax(decoder_output.squeeze(1), dim=1)\n        \n                        topk_log_probs, topk_indices = log_probs.topk(beam_width)\n        \n                        for k in range(beam_width):\n                            next_token = topk_indices[0][k].item()\n                            next_score = score + topk_log_probs[0][k].item()\n                            new_seq = seq + [next_token]\n                            new_beam.append((new_seq, next_score, hidden_next))\n        \n                    # Keep top `beam_width` beams with highest scores\n                    beam = sorted(new_beam, key=lambda x: x[1], reverse=True)[:beam_width]\n        \n                    # Move completed sequences out\n                    beam, completed = [], []\n                    for seq, score, hidden in new_beam:\n                        if seq[-1] == output_char2idx['<eos>']:\n                            completed_sequences.append((seq, score))\n                        else:\n                            beam.append((seq, score, hidden))\n                    beam = sorted(beam, key=lambda x: x[1], reverse=True)[:beam_width]\n        \n                # Choose best completed or best incomplete beam\n                if completed_sequences:\n                    best_seq = max(completed_sequences, key=lambda x: x[1])[0]\n                else:\n                    best_seq = max(beam, key=lambda x: x[1])[0]\n        \n                # Remove <sos> if present\n                if best_seq[0] == output_char2idx['<sos>']:\n                    best_seq = best_seq[1:]\n        \n                # Compare prediction with target\n                target_seq = target_tensor[0, 1:].tolist()\n                pad_idx = output_char2idx['<pad>']\n        \n                # Token accuracy\n                for pred_token, tgt_token in zip(best_seq, target_seq):\n                    if tgt_token == pad_idx:\n                        break\n                    if pred_token == tgt_token:\n                        correct_tokens += 1\n                    total_tokens += 1\n        \n                # Sequence accuracy\n                target_trimmed = [t for t in target_seq if t != pad_idx]\n                best_seq_trimmed = best_seq[:len(target_trimmed)]\n                if best_seq_trimmed == target_trimmed:\n                    correct_sequences += 1\n        \n                # Optional print\n                # predicted_word = indices_to_words([best_seq], output_idx2char)[0]\n                # actual_word = indices_to_words([target_trimmed], output_idx2char)[0]\n                # # print(f\"Predicted: {predicted_word.ljust(20)} | Actual: {actual_word}\")\n        \n        sequence_accuracy = correct_sequences / total_sequences if total_sequences > 0 else 0\n        token_accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0\n        \n        print(f\"Token Accuracy: {token_accuracy:.4f}\")\n        print(f\"Sequence Accuracy: {sequence_accuracy:.4f}\")\n        \n        wandb.log({\n            \"epoch\": epoch + 1,\n            \"train_loss\": avg_loss,\n            \"token_accuracy\": token_accuracy,\n            \"sequence_accuracy\": sequence_accuracy\n        })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:59:19.783161Z","iopub.execute_input":"2025-05-19T10:59:19.783376Z","iopub.status.idle":"2025-05-19T10:59:19.799839Z","shell.execute_reply.started":"2025-05-19T10:59:19.783360Z","shell.execute_reply":"2025-05-19T10:59:19.799283Z"}},"outputs":[],"execution_count":104},{"cell_type":"code","source":"sweep_id = wandb.sweep(sweep_config, project=\"DA6401 Assign3 - Attention\")\nwandb.agent(sweep_id, function=train, count=20)\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:59:19.800531Z","iopub.execute_input":"2025-05-19T10:59:19.800774Z"}},"outputs":[{"name":"stdout","text":"Create sweep with ID: jbseuhp0\nSweep URL: https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/sweeps/jbseuhp0\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 51rzlxsd with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_105926-51rzlxsd</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/runs/51rzlxsd' target=\"_blank\">clear-sweep-1</a></strong> to <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/sweeps/jbseuhp0' target=\"_blank\">https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/sweeps/jbseuhp0</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention' target=\"_blank\">https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/sweeps/jbseuhp0' target=\"_blank\">https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/sweeps/jbseuhp0</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/runs/51rzlxsd' target=\"_blank\">https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/runs/51rzlxsd</a>"},"metadata":{}},{"name":"stderr","text":"Epoch 1/20: 100%|██████████| 87/87 [00:04<00:00, 18.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20 Train Loss: 3.6399\nToken Accuracy: 0.0019\nSequence Accuracy: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/20: 100%|██████████| 87/87 [00:04<00:00, 20.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/20 Train Loss: 2.7781\nToken Accuracy: 0.0510\nSequence Accuracy: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/20: 100%|██████████| 87/87 [00:03<00:00, 23.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/20 Train Loss: 2.5443\nToken Accuracy: 0.0573\nSequence Accuracy: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/20: 100%|██████████| 87/87 [00:03<00:00, 22.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/20 Train Loss: 2.4011\nToken Accuracy: 0.0986\nSequence Accuracy: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/20: 100%|██████████| 87/87 [00:03<00:00, 24.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/20 Train Loss: 2.3130\nToken Accuracy: 0.1150\nSequence Accuracy: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/20: 100%|██████████| 87/87 [00:03<00:00, 23.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/20 Train Loss: 2.2412\nToken Accuracy: 0.1340\nSequence Accuracy: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/20: 100%|██████████| 87/87 [00:03<00:00, 23.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/20 Train Loss: 2.2057\nToken Accuracy: 0.1347\nSequence Accuracy: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/20: 100%|██████████| 87/87 [00:03<00:00, 23.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/20 Train Loss: 2.1622\nToken Accuracy: 0.1359\nSequence Accuracy: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/20: 100%|██████████| 87/87 [00:03<00:00, 23.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/20 Train Loss: 2.1374\nToken Accuracy: 0.1331\nSequence Accuracy: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/20: 100%|██████████| 87/87 [00:03<00:00, 24.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/20 Train Loss: 2.1138\nToken Accuracy: 0.1384\nSequence Accuracy: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/20: 100%|██████████| 87/87 [00:03<00:00, 23.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/20 Train Loss: 2.0851\nToken Accuracy: 0.1379\nSequence Accuracy: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/20: 100%|██████████| 87/87 [00:03<00:00, 22.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12/20 Train Loss: 2.0765\nToken Accuracy: 0.1373\nSequence Accuracy: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/20: 100%|██████████| 87/87 [00:03<00:00, 23.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13/20 Train Loss: 2.0467\nToken Accuracy: 0.1398\nSequence Accuracy: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/20: 100%|██████████| 87/87 [00:03<00:00, 22.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14/20 Train Loss: 2.0161\nToken Accuracy: 0.1378\nSequence Accuracy: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/20: 100%|██████████| 87/87 [00:03<00:00, 23.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15/20 Train Loss: 2.0196\nToken Accuracy: 0.1423\nSequence Accuracy: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/20: 100%|██████████| 87/87 [00:03<00:00, 23.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16/20 Train Loss: 1.9785\nToken Accuracy: 0.1429\nSequence Accuracy: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/20: 100%|██████████| 87/87 [00:03<00:00, 22.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17/20 Train Loss: 1.9757\nToken Accuracy: 0.1514\nSequence Accuracy: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/20: 100%|██████████| 87/87 [00:03<00:00, 22.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18/20 Train Loss: 1.9725\nToken Accuracy: 0.1500\nSequence Accuracy: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/20: 100%|██████████| 87/87 [00:03<00:00, 22.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19/20 Train Loss: 1.9696\nToken Accuracy: 0.1440\nSequence Accuracy: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/20: 100%|██████████| 87/87 [00:03<00:00, 23.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20/20 Train Loss: 1.9456\nToken Accuracy: 0.1501\nSequence Accuracy: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>sequence_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>token_accuracy</td><td>▁▃▄▆▆▇▇▇▇▇▇▇▇▇██████</td></tr><tr><td>train_loss</td><td>█▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>sequence_accuracy</td><td>0</td></tr><tr><td>token_accuracy</td><td>0.15007</td></tr><tr><td>train_loss</td><td>1.94556</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">clear-sweep-1</strong> at: <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/runs/51rzlxsd' target=\"_blank\">https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/runs/51rzlxsd</a><br> View project at: <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention' target=\"_blank\">https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_105926-51rzlxsd/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zv9eabtl with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_113029-zv9eabtl</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/runs/zv9eabtl' target=\"_blank\">sweet-sweep-2</a></strong> to <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/sweeps/jbseuhp0' target=\"_blank\">https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/sweeps/jbseuhp0</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention' target=\"_blank\">https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/sweeps/jbseuhp0' target=\"_blank\">https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/sweeps/jbseuhp0</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/runs/zv9eabtl' target=\"_blank\">https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/runs/zv9eabtl</a>"},"metadata":{}},{"name":"stderr","text":"Epoch 1/20: 100%|██████████| 87/87 [00:03<00:00, 22.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20 Train Loss: 2.5700\nToken Accuracy: 0.1523\nSequence Accuracy: 0.0005\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/20: 100%|██████████| 87/87 [00:03<00:00, 23.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/20 Train Loss: 1.9020\nToken Accuracy: 0.1743\nSequence Accuracy: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/20: 100%|██████████| 87/87 [00:03<00:00, 23.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/20 Train Loss: 1.6536\nToken Accuracy: 0.1976\nSequence Accuracy: 0.0011\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/20: 100%|██████████| 87/87 [00:04<00:00, 20.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/20 Train Loss: 1.4808\nToken Accuracy: 0.2326\nSequence Accuracy: 0.0032\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/20: 100%|██████████| 87/87 [00:03<00:00, 22.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/20 Train Loss: 1.3875\nToken Accuracy: 0.2569\nSequence Accuracy: 0.0060\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/20: 100%|██████████| 87/87 [00:03<00:00, 22.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/20 Train Loss: 1.2721\nToken Accuracy: 0.2607\nSequence Accuracy: 0.0101\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/20: 100%|██████████| 87/87 [00:03<00:00, 22.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/20 Train Loss: 1.2084\nToken Accuracy: 0.3066\nSequence Accuracy: 0.0204\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/20: 100%|██████████| 87/87 [00:03<00:00, 22.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/20 Train Loss: 1.1029\nToken Accuracy: 0.3242\nSequence Accuracy: 0.0275\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/20: 100%|██████████| 87/87 [00:04<00:00, 21.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/20 Train Loss: 1.0350\nToken Accuracy: 0.3687\nSequence Accuracy: 0.0461\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/20: 100%|██████████| 87/87 [00:03<00:00, 22.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/20 Train Loss: 0.9506\nToken Accuracy: 0.3967\nSequence Accuracy: 0.0592\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/20: 100%|██████████| 87/87 [00:04<00:00, 21.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/20 Train Loss: 0.8838\nToken Accuracy: 0.4328\nSequence Accuracy: 0.0773\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/20: 100%|██████████| 87/87 [00:03<00:00, 23.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12/20 Train Loss: 0.8075\nToken Accuracy: 0.4677\nSequence Accuracy: 0.0982\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/20: 100%|██████████| 87/87 [00:03<00:00, 22.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13/20 Train Loss: 0.7545\nToken Accuracy: 0.5105\nSequence Accuracy: 0.1205\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/20: 100%|██████████| 87/87 [00:03<00:00, 22.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14/20 Train Loss: 0.6948\nToken Accuracy: 0.5295\nSequence Accuracy: 0.1329\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/20: 100%|██████████| 87/87 [00:03<00:00, 23.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15/20 Train Loss: 0.6581\nToken Accuracy: 0.5315\nSequence Accuracy: 0.1450\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/20: 100%|██████████| 87/87 [00:03<00:00, 23.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16/20 Train Loss: 0.6086\nToken Accuracy: 0.5704\nSequence Accuracy: 0.1778\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/20: 100%|██████████| 87/87 [00:03<00:00, 23.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17/20 Train Loss: 0.5788\nToken Accuracy: 0.5792\nSequence Accuracy: 0.1870\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/20: 100%|██████████| 87/87 [00:03<00:00, 22.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18/20 Train Loss: 0.5507\nToken Accuracy: 0.5900\nSequence Accuracy: 0.2012\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/20: 100%|██████████| 87/87 [00:03<00:00, 23.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19/20 Train Loss: 0.5152\nToken Accuracy: 0.6135\nSequence Accuracy: 0.2214\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/20: 100%|██████████| 87/87 [00:03<00:00, 23.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20/20 Train Loss: 0.4963\nToken Accuracy: 0.6218\nSequence Accuracy: 0.2230\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>sequence_accuracy</td><td>▁▁▁▁▁▁▂▂▂▃▃▄▅▅▆▇▇▇██</td></tr><tr><td>token_accuracy</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▆▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>sequence_accuracy</td><td>0.22304</td></tr><tr><td>token_accuracy</td><td>0.62176</td></tr><tr><td>train_loss</td><td>0.49634</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">sweet-sweep-2</strong> at: <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/runs/zv9eabtl' target=\"_blank\">https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/runs/zv9eabtl</a><br> View project at: <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention' target=\"_blank\">https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_113029-zv9eabtl/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: l3cjx382 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_115402-l3cjx382</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/runs/l3cjx382' target=\"_blank\">polar-sweep-3</a></strong> to <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/sweeps/jbseuhp0' target=\"_blank\">https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/sweeps/jbseuhp0</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention' target=\"_blank\">https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/sweeps/jbseuhp0' target=\"_blank\">https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/sweeps/jbseuhp0</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/runs/l3cjx382' target=\"_blank\">https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/runs/l3cjx382</a>"},"metadata":{}},{"name":"stderr","text":"Epoch 1/20: 100%|██████████| 87/87 [00:04<00:00, 21.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20 Train Loss: 2.6200\nToken Accuracy: 0.1390\nSequence Accuracy: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/20: 100%|██████████| 87/87 [00:03<00:00, 22.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/20 Train Loss: 1.9260\nToken Accuracy: 0.1797\nSequence Accuracy: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/20: 100%|██████████| 87/87 [00:04<00:00, 20.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/20 Train Loss: 1.6661\nToken Accuracy: 0.2153\nSequence Accuracy: 0.0014\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/20: 100%|██████████| 87/87 [00:03<00:00, 22.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/20 Train Loss: 1.4932\nToken Accuracy: 0.2412\nSequence Accuracy: 0.0014\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/20: 100%|██████████| 87/87 [00:03<00:00, 22.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/20 Train Loss: 1.3230\nToken Accuracy: 0.2995\nSequence Accuracy: 0.0158\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/20: 100%|██████████| 87/87 [00:03<00:00, 22.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/20 Train Loss: 1.1963\nToken Accuracy: 0.3321\nSequence Accuracy: 0.0246\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/20: 100%|██████████| 87/87 [00:03<00:00, 22.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/20 Train Loss: 1.0812\nToken Accuracy: 0.3656\nSequence Accuracy: 0.0406\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/20: 100%|██████████| 87/87 [00:04<00:00, 20.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/20 Train Loss: 0.9735\nToken Accuracy: 0.3938\nSequence Accuracy: 0.0500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/20: 100%|██████████| 87/87 [00:03<00:00, 22.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/20 Train Loss: 0.9006\nToken Accuracy: 0.4415\nSequence Accuracy: 0.0833\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/20: 100%|██████████| 87/87 [00:03<00:00, 22.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/20 Train Loss: 0.8128\nToken Accuracy: 0.4890\nSequence Accuracy: 0.1111\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/20: 100%|██████████| 87/87 [00:04<00:00, 21.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/20 Train Loss: 0.7282\nToken Accuracy: 0.5233\nSequence Accuracy: 0.1365\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/20: 100%|██████████| 87/87 [00:03<00:00, 22.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12/20 Train Loss: 0.6653\nToken Accuracy: 0.5658\nSequence Accuracy: 0.1705\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/20: 100%|██████████| 87/87 [00:04<00:00, 21.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13/20 Train Loss: 0.6135\nToken Accuracy: 0.5926\nSequence Accuracy: 0.1976\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/20: 100%|██████████| 87/87 [00:03<00:00, 22.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14/20 Train Loss: 0.5689\nToken Accuracy: 0.6043\nSequence Accuracy: 0.2129\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/20: 100%|██████████| 87/87 [00:03<00:00, 22.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15/20 Train Loss: 0.5349\nToken Accuracy: 0.6223\nSequence Accuracy: 0.2315\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/20: 100%|██████████| 87/87 [00:03<00:00, 22.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16/20 Train Loss: 0.5081\nToken Accuracy: 0.6362\nSequence Accuracy: 0.2497\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/20: 100%|██████████| 87/87 [00:03<00:00, 22.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17/20 Train Loss: 0.4814\nToken Accuracy: 0.6208\nSequence Accuracy: 0.2441\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/20: 100%|██████████| 87/87 [00:04<00:00, 21.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18/20 Train Loss: 0.4642\nToken Accuracy: 0.6529\nSequence Accuracy: 0.2669\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/20: 100%|██████████| 87/87 [00:03<00:00, 22.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19/20 Train Loss: 0.4457\nToken Accuracy: 0.6610\nSequence Accuracy: 0.2774\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/20: 100%|██████████| 87/87 [00:03<00:00, 22.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20/20 Train Loss: 0.4294\nToken Accuracy: 0.6656\nSequence Accuracy: 0.2866\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>sequence_accuracy</td><td>▁▁▁▁▁▂▂▂▃▄▄▅▆▆▇▇▇███</td></tr><tr><td>token_accuracy</td><td>▁▂▂▂▃▄▄▄▅▆▆▇▇▇▇█▇███</td></tr><tr><td>train_loss</td><td>█▆▅▄▄▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>sequence_accuracy</td><td>0.2866</td></tr><tr><td>token_accuracy</td><td>0.66563</td></tr><tr><td>train_loss</td><td>0.42938</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">polar-sweep-3</strong> at: <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/runs/l3cjx382' target=\"_blank\">https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/runs/l3cjx382</a><br> View project at: <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention' target=\"_blank\">https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_115402-l3cjx382/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: i9ywlq6w with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_122547-i9ywlq6w</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/runs/i9ywlq6w' target=\"_blank\">super-sweep-4</a></strong> to <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/sweeps/jbseuhp0' target=\"_blank\">https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/sweeps/jbseuhp0</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention' target=\"_blank\">https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/sweeps/jbseuhp0' target=\"_blank\">https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/sweeps/jbseuhp0</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/runs/i9ywlq6w' target=\"_blank\">https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/runs/i9ywlq6w</a>"},"metadata":{}},{"name":"stderr","text":"Epoch 1/20: 100%|██████████| 87/87 [00:03<00:00, 22.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20 Train Loss: 3.2852\nToken Accuracy: 0.0786\nSequence Accuracy: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/20: 100%|██████████| 87/87 [00:04<00:00, 21.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/20 Train Loss: 2.5320\nToken Accuracy: 0.0978\nSequence Accuracy: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/20: 100%|██████████| 87/87 [00:04<00:00, 21.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/20 Train Loss: 2.3811\nToken Accuracy: 0.1165\nSequence Accuracy: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/20: 100%|██████████| 87/87 [00:03<00:00, 22.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/20 Train Loss: 2.2679\nToken Accuracy: 0.1393\nSequence Accuracy: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/20: 100%|██████████| 87/87 [00:04<00:00, 21.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/20 Train Loss: 2.1608\nToken Accuracy: 0.1558\nSequence Accuracy: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/20: 100%|██████████| 87/87 [00:03<00:00, 22.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/20 Train Loss: 2.0643\nToken Accuracy: 0.1543\nSequence Accuracy: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/20: 100%|██████████| 87/87 [00:04<00:00, 20.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/20 Train Loss: 1.9684\nToken Accuracy: 0.1668\nSequence Accuracy: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/20: 100%|██████████| 87/87 [00:04<00:00, 20.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/20 Train Loss: 1.8889\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▅▆▇█</td></tr><tr><td>sequence_accuracy</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>token_accuracy</td><td>▁▃▄▆▇▇█</td></tr><tr><td>train_loss</td><td>█▄▃▃▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>7</td></tr><tr><td>sequence_accuracy</td><td>0</td></tr><tr><td>token_accuracy</td><td>0.16685</td></tr><tr><td>train_loss</td><td>1.9684</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">super-sweep-4</strong> at: <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/runs/i9ywlq6w' target=\"_blank\">https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/runs/i9ywlq6w</a><br> View project at: <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention' target=\"_blank\">https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_122547-i9ywlq6w/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0sulwwsy with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_123515-0sulwwsy</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/runs/0sulwwsy' target=\"_blank\">restful-sweep-5</a></strong> to <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/sweeps/jbseuhp0' target=\"_blank\">https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/sweeps/jbseuhp0</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention' target=\"_blank\">https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/sweeps/jbseuhp0' target=\"_blank\">https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/sweeps/jbseuhp0</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/runs/0sulwwsy' target=\"_blank\">https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401%20Assign3%20-%20Attention/runs/0sulwwsy</a>"},"metadata":{}},{"name":"stderr","text":"Epoch 1/20: 100%|██████████| 173/173 [00:07<00:00, 23.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20 Train Loss: 2.4861\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# from tqdm import tqdm\n# import torch\n# import torch.nn as nn\n# from torch.utils.data import DataLoader\n# import torch.nn.functional as F\n\n# filepath_test = \"/kaggle/input/dakshina-dataset-v1-0/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\"\n# pairs_test = load_dakshina_lexicon_pairs(filepath_test)\n# dataset_test = TransliterationDataset(pairs_test, input_char2idx, output_char2idx)\n# dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n\n# filepath_val = \"/kaggle/input/dakshina-dataset-v1-0/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\"\n# pairs_val = load_dakshina_lexicon_pairs(filepath_val)\n# dataset_val = TransliterationDataset(pairs_val, input_char2idx, output_char2idx)\n# dataloader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, collate_fn=collate_fn)\n\n# dataset = TransliterationDataset(pairs, input_char2idx, output_char2idx)\n# dataloader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n\n# #  Best configuration\n# embed_size=64\n# num_encoder_layers=3\n# num_decoder_layers=3\n# hidden_size=64\n# cell_type='lstm'\n# dropout=0.4\n# batch_size=128\n# learning_rate=0.005\n# beam_size=4\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# # Initialize encoder and decoder\n# encoder = Encoder(\n#     input_vocab_size=len(input_char2idx),\n#     embed_size=embed_size,\n#     hidden_size=hidden_size,\n#     num_encoder_layers=num_encoder_layers,\n#     cell_type=cell_type,\n#     dropout=dropout\n# ).to(device)\n\n# decoder = Decoder(\n#     output_vocab_size=len(output_char2idx),\n#     embed_size=embed_size,\n#     hidden_size=hidden_size,\n#     num_decoder_layers=num_decoder_layers,\n#     cell_type=cell_type,\n#     dropout=dropout\n# ).to(device)\n\n# encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate, weight_decay=1e-5)\n# decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate, weight_decay=1e-5)\n# criterion = nn.CrossEntropyLoss(ignore_index=output_char2idx['<pad>'])\n\n# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n\n# num_epochs = 20\n# for epoch in range(num_epochs):\n#     # ======== TRAINING ========\n#     encoder.train()\n#     decoder.train()\n#     total_loss = 0\n\n#     with tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\") as pbar:\n#         for input_tensor, input_lengths, target_tensor, target_lengths in pbar:\n#             input_tensor = input_tensor.to(device)\n#             target_tensor = target_tensor.to(device)\n\n#             encoder_optimizer.zero_grad()\n#             decoder_optimizer.zero_grad()\n\n#             encoder_outputs, encoder_hidden = encoder(input_tensor, input_lengths)\n#             decoder_input = target_tensor[:, 0].unsqueeze(1)  # <sos>\n#             decoder_hidden = encoder_hidden\n\n#             loss = 0\n#             max_target_len = target_tensor.size(1)\n\n#             for t in range(1, max_target_len):\n#                 decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n#                 output = decoder_output.squeeze(1)\n#                 # print(f'output = {output}')\n#                 # print(f'target tensor = {target_tensor[:,t]}')\n#                 loss += criterion(output, target_tensor[:, t])\n#                 decoder_input = target_tensor[:, t].unsqueeze(1)  # Teacher forcing\n\n#             loss.backward()\n#             torch.nn.utils.clip_grad_norm_(encoder.parameters(), max_norm=1.0)\n#             torch.nn.utils.clip_grad_norm_(decoder.parameters(), max_norm=1.0)\n\n#             encoder_optimizer.step()\n#             decoder_optimizer.step()\n\n#             total_loss += loss.item() / (max_target_len - 1)\n\n#     avg_loss = total_loss / len(dataloader)\n#     print(f\"Epoch {epoch+1}/{num_epochs} Train Loss: {avg_loss:.4f}\")\n\n#     # ======== VALIDATION ========\n#     encoder.eval()\n#     decoder.eval()\n#     correct_sequences = 0\n#     total_sequences = 0\n#     correct_tokens = 0\n#     total_tokens = 0\n#     beam_width = beam_size  # You can change this\n#     with torch.no_grad():\n#         for input_tensor, input_lengths, target_tensor, target_lengths in dataloader_val:\n#             input_tensor = input_tensor.to(device)\n#             target_tensor = target_tensor.to(device)\n    \n#             encoder_outputs, encoder_hidden = encoder(input_tensor, input_lengths)\n#             max_target_len = target_tensor.size(1)\n#             total_sequences += 1\n    \n#             # Beam is a list of tuples: (sequence_so_far, cumulative_log_prob, decoder_hidden)\n#             beam = [([output_char2idx['<sos>']], 0.0, encoder_hidden)]\n    \n#             completed_sequences = []\n    \n#             for _ in range(1, max_target_len):\n#                 new_beam = []\n#                 for seq, score, hidden in beam:\n#                     decoder_input = torch.tensor([[seq[-1]]], device=device)\n#                     decoder_output, hidden_next = decoder(decoder_input, hidden)\n#                     log_probs = F.log_softmax(decoder_output.squeeze(1), dim=1)\n    \n#                     topk_log_probs, topk_indices = log_probs.topk(beam_width)\n    \n#                     for k in range(beam_width):\n#                         next_token = topk_indices[0][k].item()\n#                         next_score = score + topk_log_probs[0][k].item()\n#                         new_seq = seq + [next_token]\n#                         new_beam.append((new_seq, next_score, hidden_next))\n    \n#                 # Keep top `beam_width` beams with highest scores\n#                 beam = sorted(new_beam, key=lambda x: x[1], reverse=True)[:beam_width]\n    \n#                 # Move completed sequences out\n#                 beam, completed = [], []\n#                 for seq, score, hidden in new_beam:\n#                     if seq[-1] == output_char2idx['<eos>']:\n#                         completed_sequences.append((seq, score))\n#                     else:\n#                         beam.append((seq, score, hidden))\n#                 beam = sorted(beam, key=lambda x: x[1], reverse=True)[:beam_width]\n    \n#             # Choose best completed or best incomplete beam\n#             if completed_sequences:\n#                 best_seq = max(completed_sequences, key=lambda x: x[1])[0]\n#             else:\n#                 best_seq = max(beam, key=lambda x: x[1])[0]\n    \n#             # Remove <sos> if present\n#             if best_seq[0] == output_char2idx['<sos>']:\n#                 best_seq = best_seq[1:]\n    \n#             # Compare prediction with target\n#             target_seq = target_tensor[0, 1:].tolist()\n#             pad_idx = output_char2idx['<pad>']\n    \n#             # Token accuracy\n#             for pred_token, tgt_token in zip(best_seq, target_seq):\n#                 if tgt_token == pad_idx:\n#                     break\n#                 if pred_token == tgt_token:\n#                     correct_tokens += 1\n#                 total_tokens += 1\n    \n#             # Sequence accuracy\n#             target_trimmed = [t for t in target_seq if t != pad_idx]\n#             best_seq_trimmed = best_seq[:len(target_trimmed)]\n#             if best_seq_trimmed == target_trimmed:\n#                 correct_sequences += 1\n    \n#             # Optional print\n#             # predicted_word = indices_to_words([best_seq], output_idx2char)[0]\n#             # actual_word = indices_to_words([target_trimmed], output_idx2char)[0]\n#             # # print(f\"Predicted: {predicted_word.ljust(20)} | Actual: {actual_word}\")\n    \n#     sequence_accuracy = correct_sequences / total_sequences if total_sequences > 0 else 0\n#     token_accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0\n    \n#     print(f\"Token Accuracy: {token_accuracy:.4f}\")\n#     print(f\"Sequence Accuracy: {sequence_accuracy:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # ======== TEST ========\n# encoder.eval()\n# decoder.eval()\n# correct_sequences = 0\n# total_sequences = 0\n# correct_tokens = 0\n# total_tokens = 0\n# beam_width = beam_size  # You can change this\n# result=[]\n# with torch.no_grad():\n#     for input_tensor, input_lengths, target_tensor, target_lengths in dataloader_test:\n#         input_tensor = input_tensor.to(device)\n#         target_tensor = target_tensor.to(device)\n\n#         encoder_outputs, encoder_hidden = encoder(input_tensor, input_lengths)\n#         max_target_len = target_tensor.size(1)\n#         total_sequences += 1\n\n#         # Beam is a list of tuples: (sequence_so_far, cumulative_log_prob, decoder_hidden)\n#         beam = [([output_char2idx['<sos>']], 0.0, encoder_hidden)]\n\n#         completed_sequences = []\n\n#         for _ in range(1, max_target_len):\n#             new_beam = []\n#             for seq, score, hidden in beam:\n#                 decoder_input = torch.tensor([[seq[-1]]], device=device)\n#                 decoder_output, hidden_next = decoder(decoder_input, hidden)\n#                 log_probs = F.log_softmax(decoder_output.squeeze(1), dim=1)\n\n#                 topk_log_probs, topk_indices = log_probs.topk(beam_width)\n\n#                 for k in range(beam_width):\n#                     next_token = topk_indices[0][k].item()\n#                     next_score = score + topk_log_probs[0][k].item()\n#                     new_seq = seq + [next_token]\n#                     new_beam.append((new_seq, next_score, hidden_next))\n\n#             # Keep top `beam_width` beams with highest scores\n#             beam = sorted(new_beam, key=lambda x: x[1], reverse=True)[:beam_width]\n\n#             # Move completed sequences out\n#             beam, completed = [], []\n#             for seq, score, hidden in new_beam:\n#                 if seq[-1] == output_char2idx['<eos>']:\n#                     completed_sequences.append((seq, score))\n#                 else:\n#                     beam.append((seq, score, hidden))\n#             beam = sorted(beam, key=lambda x: x[1], reverse=True)[:beam_width]\n\n#         # Choose best completed or best incomplete beam\n#         if completed_sequences:\n#             best_seq = max(completed_sequences, key=lambda x: x[1])[0]\n#         else:\n#             best_seq = max(beam, key=lambda x: x[1])[0]\n\n#         # Remove <sos> if present\n#         if best_seq[0] == output_char2idx['<sos>']:\n#             best_seq = best_seq[1:]\n        \n#         # Compare prediction with target\n#         target_seq = target_tensor[0, 1:].tolist()\n#         pad_idx = output_char2idx['<pad>']\n\n#         # Token accuracy\n#         for pred_token, tgt_token in zip(best_seq, target_seq):\n#             if tgt_token == pad_idx:\n#                 break\n#             if pred_token == tgt_token:\n#                 correct_tokens += 1\n#             total_tokens += 1\n\n#         # Sequence accuracy\n#         target_trimmed = [t for t in target_seq if t != pad_idx]\n#         best_seq_trimmed = best_seq[:len(target_trimmed)]\n#         if best_seq_trimmed == target_trimmed:\n#             correct_sequences += 1\n        \n#         if best_seq[-1] == output_char2idx['<eos>']:\n#             best_seq = best_seq[:best_seq.index(output_char2idx['<eos>'])]\n#         predicted_word=''.join(output_idx2char[i] for i in best_seq)\n#         target_seq = target_tensor.tolist() if hasattr(target_tensor, 'tolist') else target_tensor\n#         if isinstance(target_seq[0], list):\n#             target_seq = target_seq[0]\n#         # Remove <sos> and truncate at <eos> if present\n#         if target_seq[0] == output_char2idx['<sos>']:\n#             target_seq = target_seq[1:]\n#         if output_char2idx.get('<eos>') in target_seq:\n#             target_seq = target_seq[:target_seq.index(output_char2idx['<eos>'])]\n        \n#         target_word = ''.join(output_idx2char[i] for i in target_seq)\n        \n#         input_seq = input_tensor.tolist() if hasattr(input_tensor, 'tolist') else input_tensor\n#         if isinstance(input_seq[0], list):\n#             input_seq = input_seq[0]\n\n#         if input_seq[0] == input_char2idx['<sos>']:\n#             input_seq = input_seq[1:]\n#         if input_char2idx.get('<eos>') in input_seq:\n#             input_seq = input_seq[:input_seq.index(input_char2idx['<eos>'])]\n#         input_word = ''.join(input_idx2char[i] for i in input_seq)\n\n#         result.append((input_word, predicted_word, target_word))\n#         # Optional print\n#         # predicted_word = indices_to_words([best_seq], output_idx2char)[0]\n#         # actual_word = indices_to_words([target_trimmed], output_idx2char)[0]\n#         # # print(f\"Predicted: {predicted_word.ljust(20)} | Actual: {actual_word}\")\n\n# sequence_accuracy = correct_sequences / total_sequences if total_sequences > 0 else 0\n# token_accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0\n# print(\"Test:\")\n# print(f\"Token Accuracy: {token_accuracy:.4f}\")\n# print(f\"Sequence Accuracy: {sequence_accuracy:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(result[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import csv\n\n# with open('predictions_vanilla.csv', 'w', newline='', encoding='utf-8') as f:\n#     writer = csv.writer(f)\n#     writer.writerow(['Input', 'Predicted', 'Target'])  # Header\n#     for t, pred, target in result:\n#         writer.writerow([t, pred, target])","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}